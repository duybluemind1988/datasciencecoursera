---
title: "ML project"
name: Nguyen Ngoc Duy
output:
  html_document:
    df_print: paged
---
## Get data
```{r}
library(tidyverse)
library(data.table)
library(caret)
library(recipes)
```


```{r}

training <- fread("pml-training.csv")
testing <- fread("pml-testing.csv")
head(training)
head(testing)
```
```{r}
dim(training)
```


```{r}
table(training$classe)
```
Check NA
```{r}
# Check which column have na value > 50 %
col_high_na_value<-training[, which(colMeans(is.na(training)) > 0)]
col_high_na_value
```

## Prepare data for machine learning

```{r}
# all character columns to factor:
training <- mutate_if(training, is.character, as.factor)
testing <- mutate_if(testing, is.character, as.factor)
#data$Attrition<-ifelse(data$Attrition=="Yes", 1, 0)
# Create training (80%) and test (20%) sets for the 
set.seed(430)
split = createDataPartition(training$classe, p =0.8, list = FALSE)
train = training[split, ]
valid = training[-split, ]
dim(train)
dim(valid)
```

```{r}
table(train$classe)
table(valid$classe)
```
While your projectâ€™s needs may vary, here is a suggested order of potential steps that should work for most problems:

Filter out zero or near-zero variance features.
Perform imputation if required.
Normalize to resolve numeric feature skewness.
Standardize (center and scale) numeric features.

```{r}
blueprint <- recipe(classe ~ ., data = train) %>%
  step_nzv(all_numeric(), -all_outcomes())  %>% #Remove near-zero variance features like sex, yes/no...
  step_YeoJohnson(all_numeric(),-all_outcomes()) %>% # Remove skewness
  step_center(all_numeric(), -all_outcomes()) %>% # center 
  step_scale(all_numeric(), -all_outcomes()) #%>% # scale
blueprint
```
Next, we need to train this blueprint on some training data. Remember, there are many feature engineering steps that we do not want to train on the test data (e.g., standardize and PCA) as this would create data leakage. So in this step we estimate these parameters based on the training data of interest.
```{r}
prepare <- prep(blueprint, training = train)
```

Lastly, we can apply our blueprint to new data (e.g., the training data or future test data) with bake()

```{r}
baked_train <- bake(prepare, new_data = train)
baked_valid <- bake(prepare, new_data = valid)
baked_test <- bake(prepare, new_data = testing)
head(baked_train)
```

##  H2O  machine learning model
I choose H2O machine learning model because it is very good accuracy and very fast

```{r}
library(h2o)
h2o.no_progress()
h2o.init()
```


```{r}
# convert training data to h2o object
train_h2o <- as.h2o(baked_train)
valid_h2o <- as.h2o(baked_valid)
test_h2o <- as.h2o(baked_test)
# set the response column to Sale_Price
response <- "classe"
n_features <- length(setdiff(names(baked_train), "classe"))
# set the predictor names
predictors <- setdiff(colnames(baked_train), response)
```

Training 3 models
```{r}
# XGBOOST model
h2o_xgboost <- h2o.xgboost(
    x = predictors, 
    y = response,
    training_frame = train_h2o, 
    #family='multinomial',
    seed = 123
)

# Gradient boosting 
h2o_gbm <- h2o.gbm(
    x = predictors, 
    y = response,
    training_frame = train_h2o, 
    #family='multinomial',
    #ntrees = n_features * 10, # very slow if high dim, not pca
    #learn_rate=0.2,
    seed = 123
)
# Generalized linear model
h2o_glm <- h2o.glm(
    x = predictors, 
    y = response,
    training_frame = train_h2o, 
    family='multinomial',
    #lambda_search = TRUE,
    seed = 123
)
```


## Validation set performance:
```{r}
#which( colnames(valid_h2o)=="classe" ) #127
#valid_h2o<-valid_h2o[,-127] 
#which( colnames(valid_h2o)=="classe" )
```

```{r}
# Check model performance in test set
h2o.performance(h2o_xgboost, newdata = valid_h2o)
```
Note: accuracy for 5 class: 99.97%
```{r}
h2o.performance(h2o_gbm, newdata = valid_h2o)
```
Note: accuracy for 5 class: 100%

```{r}
h2o.performance(h2o_glm, newdata = valid_h2o)
```
Note: accuracy for 5 class: 99.26 %

Best validation performance: h2o_gbm model

## Test set performance:

Apply h2o_gbm model to test set:
```{r}
test_h2o
```

```{r}
result <- h2o.predict(h2o_gbm, newdata = test_h2o)
head(result,20)
```

```{r}
result <- h2o.predict(h2o_xgboost, newdata = test_h2o)
head(result,20)
```
```{r}
result <- h2o.predict(h2o_glm, newdata = test_h2o)
head(result,20)
```

### Explain model

```{r}
h2o.varimp_plot(h2o_xgboost)
h2o.varimp_plot(h2o_gbm)
h2o.varimp_plot(h2o_glm)
```
```{r}
STOP HERE
```

# OTHER METHOD
## Try Random forest model
```{r}
#library(doParallel)	
#cl <-makePSOCKcluster(5)	
#registerDoParallel(cl)	
```

```{r}
cv <- trainControl(
  method = "cv", 
  number = 4)
start_time <- lubridate::minute(Sys.time())

modelRF <- train(classe ~ ., data = train, method = "rf",#trControl=cv
                 )

end_time <- lubridate::minute(Sys.time())
time <- (end_time - start_time)
print(time) # 11 min cho 5 cv, 2 min cho 1 cv
```


```{r}
predictRF <- predict(modelRF, valid)
confusionMatrix(factor(valid$classe), predictRF)
```

## Predicting The Manner of Exercise for Test Data Set
```{r}
predict(modelRF, testing)
```
[1] B A B A A E D B A A B C B A E E A B B B
Levels: A B C D E

## Appendix
## Try XGBOOST model (Very fast)

```{r}
library(xgboost)
```
```{r}
set.seed(430)
split = createDataPartition(training$classe, p =0.8, list = FALSE)
train = training[split, ]
valid = training[-split, ]

train <- train[, classe:=as.factor(classe)]
valid <- valid[, classe:=as.factor(classe)]
class_name_original = train[,c("classe")]
train <- train[, classe:=as.integer(classe)-1]
valid <- valid[, classe:=as.integer(classe)-1]

train_matrix <- as.matrix(train[,-c("classe")])
train_label_matrix <- as.matrix(train[,c("classe")]) 

valid_matrix <- as.matrix(valid[,-c("classe")])
valid_label_matrix <- as.matrix(valid[,c("classe")]) 

test_matrix <- as.matrix(testing)
```

```{r}

```


```{r}
dtrain <- xgb.DMatrix(data =train_matrix, label =train_label_matrix )
dvalid <- xgb.DMatrix(data =valid_matrix, label =valid_label_matrix )
dtest <- xgb.DMatrix(data =test_matrix)
```

```{r}
#num_class = length(levels(species))
num_class = 5
params = list(
  booster="gbtree",
  eta=0.001,
  max_depth=5,
  gamma=3,
  subsample=0.75,
  colsample_bytree=1,
  objective="multi:softprob",
  eval_metric="mlogloss", #Multiclass logloss.
  num_class=num_class
)
```

```{r}
# Train the XGBoost classifer
xgb.fit=xgb.train(
  params=params,
  data=dtrain,
  nrounds=100,
  nthreads=2,
  early_stopping_rounds=10,
  watchlist=list(val1=dtrain,val2=dvalid),
  verbose=0
)

# Review the final model and results
xgb.fit
```

```{r}
# Predict outcomes with the test data
xgb.pred = predict(xgb.fit,dvalid,reshape=T)
xgb.pred = as.data.frame(xgb.pred)
colnames(xgb.pred) = levels(class_name_original)
```

```{r}
# Use the predicted label with the highest probability
xgb.pred$prediction = apply(xgb.pred,1,function(x) colnames(xgb.pred)[which.max(x)])
xgb.pred$label = levels(class_name_original)[valid+1]
```



```{r}
```


```{r}
```


```{r}
```


```{r}
```



